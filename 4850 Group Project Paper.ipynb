{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% PACKAGES INCLUDED HERE \n",
    "% DO NOT NEED TO CHANGE\n",
    "\\documentclass[conference]{IEEEtran}\n",
    "%\\IEEEoverridecommandlockouts\n",
    "% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\n",
    "\\usepackage{cite}\n",
    "\\usepackage{amsmath,amssymb,amsfonts}\n",
    "\\usepackage{algorithmic}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{textcomp}\n",
    "\\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em\n",
    "    T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}\n",
    "\\begin{document}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% TITLE GOES HERE\n",
    "\n",
    "\\title{Data Augmentation and its Effects on Audio Recognition by Neural Nets\\\\}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% AUTHOR NAMES GOES HERE\n",
    "\n",
    "\\author{\\IEEEauthorblockN{1\\textsuperscript{st} Grayson Cordell}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "glc3k@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{2\\textsuperscript{nd} Jesse Gailbreath}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "jgg2x@mtmail.mtsu.edu}\n",
    "\\linebreakand\n",
    "\\IEEEauthorblockN{4\\textsuperscript{th} Jacob Swindell}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "jds2fe@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{3\\textsuperscript{rd} Noah Norrod}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, United States \\\\\n",
    "nan2q@mtmail.mtsu.edu}\n",
    "}\n",
    "\n",
    "\\maketitle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% ABSTRACT \n",
    "\\begin{abstract}\n",
    "Small data sets pose a problem when using neural networks. Limited amounts of data can cause a litany of issues, the largest being the possibility of overfitting a model. One way to improve a model's performance is through the augmentation of the existing data set. This method allows for an artificial expansion of the data set. In our research, we compared the results of sound augmentations on a CNN-LSTM network. The model used was the \"free-spoken-digit\" data set. The unmodified data set contains 3,000 samples of speakers saying digits zero through nine. Once all augmentations were completed, we reached over 30,000 categorized samples. After testing both data sets, we were able to demonstrate that with augmentation, the performance of our network improved compared to when the non-augmented data set is run through. The augmented data set improved the model's validation accuracy by anywhere from 6 to 21 percent, with an average of around 11 percent. For smaller data sets, attempting augmentation techniques can be very advantageous.\n",
    "\\end{abstract}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% KEYWORDS\n",
    "\n",
    "\\begin{IEEEkeywords}\n",
    "neural nets, audio, data augmentation, CNN, LSTM, spectrogram.\n",
    "\\end{IEEEkeywords}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% INTRODUCTION AND BACKGROUND SECTION\n",
    "\\section{Introduction and Background}\n",
    "\n",
    "While classification using machine learning has been evolving and many different architectures exist, audio classification is often considered a more challenging and less researched area. By applying techniques and methods used for image classification, results in audio classification have been able to achieve solid results. Currently, this method is quite prevalent. There are wide varieties of voice dictation products where audio classification is utilized, but while these corporations have truly immense data sets for training, this is not always the case for researchers.\\cite{b1} Sometimes, a more niche and specific classification system may be necessary, but without proper data scraping, this proves to be difficult. Small data sets tend to result in training models that are not capable of generalization. Generalization is needed for the effective classification of unseen samples. That's where the idea of data augmentation comes into play. Artificially increasing the amount of available data by creating new instances based on the original data can help compensate for this lack of training data. The new data is created through altering an existing set of data in such a way to produce new instances.\\cite{b1} In the scope of our project, data augmentation takes the form of changing audio samples through pitch shifting, frequency suppression, additive background noise, and Randomized reverb. Pitch is the more common term for the \"frequency\" of a soundwave, or, more scientifically, how fast a soundwave modulates, in terms of pressure, between compression and rarefaction. The higher or lower the frequency, the higher or lower the pitch of the sound, respectively.\\cite{b3} In this case, we have taken the original sound files and either increased or decreased the frequency by preset and random intervals. Frequency suppression incorporates what is known as band stop, which is when a set range of frequencies from a signal is masked or silenced. The additive noise process introduced a static signal and combined it with a target signal. Our randomized reverb effect added a reverb with a randomized “room size” to the signal, creating an echoing effect. Reverb is an effect manipulated in audio production to create an illusion of differently sized spaces. For example, audio that sounds as though it was recorded in a hallway small room, or any other location where an echo could be found. By adding a reverb effect to our audio samples, we can emulate the sound of those spaces. These methods create comprehensible samples that still maintain the speaker's message. With each new sample, we can add it into our data set, and while maybe not quite as adequate as having more untouched samples thus creating a much larger data set than we originally started with.  \n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{low-high-frequency-sound-wave.jpg}}\n",
    "\\caption{Diagram showing how frequency affects a sound wave.\\cite{b4}}\n",
    "\\label{fig}\n",
    "\\end{figure}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% METHODS SECTION\n",
    "\\section{Methods}\n",
    "\n",
    "Our chosen sample set was the relatively small Free Spoken Digit Dataset (FSDD).\\cite{b5} We used the original set that contains just 3,000 samples of waveform audio (denoted by the file extension “.wav”) files from 6 speakers repeating the individual digits 0 through 9 multiple times. All work was performed using Python in Jupyter Notebook format. Augmentations to the samples were done with WavAugment, which is recognized for use with speech.\\cite{b6} 10 unique augmentations were made to each sample to grow the data set to 33,000 samples.\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{effect_table.png}}\n",
    "\\caption{Table showing the different effects we used to augment our samples.}\n",
    "\\label{fig}\n",
    "\\end{figure}\n",
    "The samples were then processed using the Librosa package in Python to convert the audio samples from .wav files to spectrograms.\\cite{b7} Spectrogram images take the audio signals and give us a visual representation of the signal's amplitude, or volume, across multiple frequencies over time. Visually, it is now possible to see what small changes to a single sound sample look like.\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{0_ADN.jpg}}\n",
    "\\caption{Spectrogram of speaker saying \"zero\" from the original data set.}\n",
    "\\label{fig}\n",
    "\\end{figure}\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{0_pitchP200.jpg}}\n",
    "\\caption{Spectrogram of speaker saying \"zero\" after being pitched up two semitones.}\n",
    "\\label{fig}\n",
    "\\end{figure}\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{0_RRsize_2.jpg}}\n",
    "\\caption{Spectrogram of speaker saying \"zero\" after randomized reverb was applied.}\n",
    "\\label{fig}\n",
    "\\end{figure}\n",
    "These three examples are directly from our data set. We can visibly see they are almost identical, but a deep learning model will be able to identify the minor differences. The shape stays consistent in both images, but there are unique shifts that have occurred that can be recognized when processed by a deep learning model. The images were then stored in batch data sets using Tensorflow. This led to the chosen architecture; a CNN-LSTM. This architecture provides improved feature extraction of the images with the ability to evaluate with respect to time. By using this architecture, we can address the spatial and temporal aspects of audio within the same neural network.\n",
    "The spectrogram images are fed into two convolutional layers, processed for normalization and dropout, and then reshaped into 3-dimensional data. From here, it is moved into a time-distributed fully-connected layer before passing off to an LSTM layer before output. The network was tested using two different amounts of inputs for the layers. This was done to make sure a network with more connections would still produce improved accuracy. The resulting output is a categorical assignment to one of the ten spoken digits. The models were run with a 70/30 training and testing split from each of the data sets. \n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{model_summary.jpg}}\n",
    "\\caption{CNN LSTM model structure.}\n",
    "\\label{fig}\n",
    "\\end{figure}\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{model_summary2.jpg}}\n",
    "\\caption{CNN LSTM model structure with reduced inputs.}\n",
    "\\label{fig}\n",
    "\\end{figure}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% RESULTS SECTION\n",
    "\\section{Results}\n",
    "\n",
    "The end result from multiple training sessions are as follows:\n",
    "\\begin{figure}[htbp]\n",
    "\\centerline{\\includegraphics[width=\\linewidth]{results_table-1.jpg}}\n",
    "\\caption{Test Accuracy and Variance results.}\n",
    "\\label{fig}\n",
    "\\end{figure}\n",
    "Upon review, it is evident that the results from the larger augmented data sets show a noticeable improvement over the original. Adjusting the layers did alter the end percentages, but the variance between the data sets is still the same. Using multiple augmentation techniques on an original smaller data set has allowed the network access to more samples, resulting in improved accuracy from the model. After performing seven individual sessions, the average variance for accuracy during testing was 11.45 percentage points in favor of the augmented data set. The most substantial variance was 21.08 percentage points in favor of the augmented set. It should be noted the augmented data set outperformed the original data set each session.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% DISCUSSION AND CONCLUSION SECTION\n",
    "\\section{Discussion and Conclusion}\n",
    "\n",
    "The rapid growth in the implementation of machine learning by such a wide range of industries and institutions has shown the need not just for reliable data, but for reliable data in large quantities. Augmentation of data, specifically audio, is proving to be an invaluable tool in achieving this goal. Using these types of methods, as in the field of natural language processing, can bolster the effectiveness of machine learning in ways that would not be possible otherwise. The testing also indicates that using augmented data sets in more complex models would still yield improved accuracy and possibly help prevent overfitting. Not only do these results solidify the value of using data augmentation for improved results, but they also represent a measurable reduction in time and resources needed to amass a quality data set. This alone makes data augmentation a smart choice for research and development whenever possible. Further study into which types of augmentation can lead to the greatest range of improvement would be beneficial. \n",
    "This alone makes data augmentation a smart choice for research and development when possible. Due to the sheer size of the model, the weights lost upon return and time constraints, we did not get the chance to test the augmented dataset model on a non-augmented testing set. Although this was not explicitly seen, we are confident that it would perform better when used on the same testing sets after seeing the current validation achieved. We encourage further study into the degree to which specific types of audio augmentations improve their models.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% REFERENCES\n",
    "% THIS IS CREATED AUTOMATICALLY\n",
    "\\bibliographystyle{IEEEtran}\n",
    "\\bibliography{References} % change if another name is used for References file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\end{document}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
